{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Feature Extraction with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def map_tags_to_features(tags):\n",
    "    \"\"\"\n",
    "    Maps tags to high-level features such as mood, energy, etc.\n",
    "    \"\"\"\n",
    "    mood_mapping = {\n",
    "        'Ambient': 'calm',\n",
    "        'Cinematic': 'epic',\n",
    "        'Looping': 'neutral',\n",
    "        'Drone': 'dark',\n",
    "        'Atmosphere': 'calm',\n",
    "        'Pad': 'soft',\n",
    "        'Drums': 'energetic',\n",
    "        'Sound-Design': 'creative'\n",
    "    }\n",
    "    \n",
    "    energy_mapping = {\n",
    "        'Drums': 0.9,\n",
    "        'Cinematic': 0.8,\n",
    "        'Looping': 0.6,\n",
    "        'Drone': 0.4,\n",
    "        'Pad': 0.5,\n",
    "        'Ambient': 0.3\n",
    "    }\n",
    "    \n",
    "    # Default values\n",
    "    mood = 'neutral'\n",
    "    energy = 0.5\n",
    "    \n",
    "    for tag in tags:\n",
    "        if tag in mood_mapping:\n",
    "            mood = mood_mapping[tag]\n",
    "        if tag in energy_mapping:\n",
    "            energy = max(energy, energy_mapping[tag])\n",
    "    \n",
    "    return mood, energy\n",
    "\n",
    "\n",
    "def extract_audio_features_with_metadata(audio_path, sr=22050, metadata=None):\n",
    "    \"\"\"\n",
    "    Extracts audio features using Librosa and combines them with metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_path (str): Path to the audio file.\n",
    "    sr (int): Sampling rate.\n",
    "    metadata (dict): High-level feature metadata.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of extracted audio and metadata features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(audio_path):\n",
    "            y, sr = librosa.load(audio_path, sr=sr)\n",
    "        else:\n",
    "            print(f\"File not found: {audio_path}\")\n",
    "            return None\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Extract low-level audio features with Librosa\n",
    "        bpm, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        features['bpm'] = bpm if isinstance(bpm, (float, int)) else bpm.item()\n",
    "\n",
    "        features['zero_crossing_rate'] = float(np.mean(librosa.feature.zero_crossing_rate(y)))\n",
    "        features['spectral_centroid'] = float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))\n",
    "        features['spectral_bandwidth'] = float(np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)))\n",
    "        features['rms_energy'] = float(np.mean(librosa.feature.rms(y=y)))\n",
    "\n",
    "        features['mfccs'] = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
    "        features['chroma'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)\n",
    "\n",
    "        # Map metadata to relevant features (these are placeholders for model learning)\n",
    "        if metadata:\n",
    "            features.update({\n",
    "                'kick_intensity': metadata.get('Kick', 0),\n",
    "                'snare_intensity': metadata.get('Snare', 0),\n",
    "                'hihat_presence': metadata.get('Hi-hat', 0),\n",
    "                'percussion': metadata.get('Percussion', 0),\n",
    "                'mood': metadata.get('Mood', 'neutral'),\n",
    "                'timbre': metadata.get('Timbre', 'balanced'),\n",
    "                'energy': metadata.get('Energy', 0),\n",
    "                'danceability': metadata.get('Danceability', 0),\n",
    "                'global_loudness': metadata.get('Global Loudness', 0),\n",
    "                'dynamic_complexity': metadata.get('Dynamic Complexity', 0),\n",
    "                'valence': metadata.get('Valence', 'neutral'),\n",
    "                'arousal': metadata.get('Arousal', 'calm'),\n",
    "                'pitch_confidence': metadata.get('Pitch Confidence', 0),\n",
    "                'harmonicity': metadata.get('Harmonicity', 0),\n",
    "                'texture_density': metadata.get('Texture Density', 0),\n",
    "                'articulation': metadata.get('Articulation', 'smooth')\n",
    "            })\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or extracting features from file: {audio_path}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def print_feature_info(features):\n",
    "    \"\"\"\n",
    "    Print detailed information about extracted features.\n",
    "    \n",
    "    Parameters:\n",
    "    features (dict): Extracted audio features.\n",
    "    \"\"\"\n",
    "    for key, value in features.items():\n",
    "        print(f\"Feature: {key}\")\n",
    "        print(f\"Type: {type(value)}\")\n",
    "        \n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"Value: {value}\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"Shape: {value.shape}\")\n",
    "            print(f\"First few values: {value[:5]}\")  # Display the first 5 elements\n",
    "        else:\n",
    "            print(f\"Value: {value}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "audio_path = \"D:/LV-NTF+LoopGAN/data/FSL10K-trimmed/505_301.wav\"\n",
    "metadata = {\n",
    "    \"Kick\": 0.8, \n",
    "    \"Snare\": 0.7, \n",
    "    \"Hi-hat\": 0.9, \n",
    "    \"Percussion\": 0.6, \n",
    "    \"Mood\": \"happy\", \n",
    "    \"Timbre\": \"bright\", \n",
    "    \"Energy\": 0.85, \n",
    "    \"Danceability\": 0.9\n",
    "}\n",
    "\n",
    "# Extract features\n",
    "audio_features = extract_audio_features_with_metadata(audio_path, metadata=metadata)\n",
    "# Optionally, uncomment this for debugging feature extraction\n",
    "# print_feature_info(audio_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture (Encoder and Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Linear layers to downsample the input to latent space\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)  # Latent space: Mean (mu)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)  # Latent space: Log-variance (logvar)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to get mu and logvar for the latent space.\n",
    "        \"\"\"\n",
    "        h = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)  # Get mean for latent space\n",
    "        logvar = self.fc_logvar(h)  # Get log variance for latent space\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Linear layers to upsample the latent space back to the original input space\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass to decode from latent space z back to the input space.\n",
    "        \"\"\"\n",
    "        h = torch.relu(self.fc1(z))  # Apply ReLU activation\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))  # Sigmoid to normalize output (between 0 and 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StyleGAN component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        # Fully connected layers to process the latent space\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # StyleGAN adds complexity by introducing deeper fully connected layers\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))  # Sigmoid to match mel-spectrogram normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MelGAN Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelGAN, self).__init__()\n",
    "        # Transposed convolutions to generate audio from mel-spectrogram\n",
    "        self.conv1 = nn.ConvTranspose1d(1, 128, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(64, 1, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # MelGAN takes in a mel-spectrogram and generates waveforms using transpose convolutions\n",
    "        if x.shape[1] != 1:  \n",
    "            x = x.unsqueeze(1)  # Ensure it has 1 channel for ConvTranspose1d\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return torch.tanh(self.conv3(x))  # Tanh ensures the output waveform is within [-1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DrumLoopVAEStyleGAN Model\n",
    "- Implementing LV-NTF for Tensor Factorization of Audio Data\n",
    "- Integrating LV-NTF in the VAE-StyleGAN Pipeline\n",
    "- Implementing LTN (Loop Transformation Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core tensor shape: (1, 64, 128)\n",
      "Factor shapes: [(1, 1), (128, 64), (256, 128)]\n"
     ]
    }
   ],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_tucker\n",
    "import numpy as np\n",
    "\n",
    "def apply_lv_ntf(mel_spec, rank=(1, 64, 128)):\n",
    "    \"\"\"\n",
    "    Apply Nonnegative Tucker Decomposition (LV-NTF) on mel-spectrogram.\n",
    "    \n",
    "    Params:\n",
    "    - mel_spec: Input mel-spectrogram (2D or 3D tensor).\n",
    "    - rank: The rank of decomposition (adjusted based on input dimensions).\n",
    "    \n",
    "    Returns:\n",
    "    - core_tensor: Core tensor from LV-NTF.\n",
    "    - factors: Factor matrices (latent factors).\n",
    "    \"\"\"\n",
    "    # Ensure input mel-spectrogram is 3D (for LV-NTF)\n",
    "    if mel_spec.ndim == 2:\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)  # Add channel dimension if 2D\n",
    "\n",
    "    # Apply non-negative Tucker decomposition\n",
    "    core_tensor, factors = non_negative_tucker(mel_spec, rank=rank, init='random', tol=10e-5)\n",
    "    \n",
    "    return core_tensor, factors\n",
    "\n",
    "# Example usage\n",
    "mel_spec = np.random.rand(128, 256)  # Sample mel-spectrogram data (2D)\n",
    "core, factors = apply_lv_ntf(mel_spec, rank=(1, 64, 128))  # Adjusted rank for 3D tensor\n",
    "\n",
    "print(\"Core tensor shape:\", core.shape)\n",
    "print(\"Factor shapes:\", [factor.shape for factor in factors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original audio shape: (1, 22050)\n",
      "Processed audio shape for librosa: (22050,)\n",
      "Time-stretched audio shape: (22050,)\n",
      "Pitch-shifted audio shape: (22050,)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import librosa\n",
    "\n",
    "class LoopTransformationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LoopTransformationNetwork, self).__init__()\n",
    "        self.time_stretch_factor = nn.Parameter(torch.tensor(1.0))  # Stretch factor, 1.0 means no stretching\n",
    "        self.pitch_shift_factor = nn.Parameter(torch.tensor(0.0))   # Pitch shift in semitones, 0 means no shift\n",
    "\n",
    "    def forward(self, audio_waveform, sample_rate=22050):\n",
    "        \"\"\"\n",
    "        Applies time-stretching and pitch-shifting to the audio loop.\n",
    "        \n",
    "        Parameters:\n",
    "        - audio_waveform (Tensor): Input waveform, expected shape (batch_size, samples).\n",
    "        - sample_rate (int): Sampling rate of the input audio.\n",
    "        \n",
    "        Returns:\n",
    "        - transformed_audio: Audio after time-stretching and pitch-shifting.\n",
    "        \"\"\"\n",
    "        # Convert tensor to NumPy array for librosa\n",
    "        audio_np = audio_waveform.cpu().numpy()\n",
    "\n",
    "        # Debug: Check the shape of the audio input\n",
    "        print(f\"Original audio shape: {audio_np.shape}\")\n",
    "\n",
    "        # Ensure audio is a 1D array for librosa\n",
    "        if audio_np.ndim == 2:\n",
    "            audio_np = audio_np[0]  # Take the first channel for simplicity (assuming mono input)\n",
    "        \n",
    "        # Debug: After flattening to ensure it's 1D\n",
    "        print(f\"Processed audio shape for librosa: {audio_np.shape}\")\n",
    "\n",
    "        # Apply time-stretching using librosa with the correct keyword argument\n",
    "        try:\n",
    "            stretched_audio = librosa.effects.time_stretch(audio_np, rate=self.time_stretch_factor.item())  # Corrected here\n",
    "            print(f\"Time-stretched audio shape: {stretched_audio.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during time-stretching: {e}\")\n",
    "            return audio_waveform  # Return original audio if stretching fails\n",
    "\n",
    "        # Apply pitch-shifting using librosa\n",
    "        try:\n",
    "            transformed_audio = librosa.effects.pitch_shift(stretched_audio, sr=sample_rate, n_steps=self.pitch_shift_factor.item())\n",
    "            print(f\"Pitch-shifted audio shape: {transformed_audio.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during pitch-shifting: {e}\")\n",
    "            return torch.tensor(stretched_audio).unsqueeze(0).to(audio_waveform.device)  # Return stretched audio if pitch shift fails\n",
    "\n",
    "        # Convert the result back to a tensor\n",
    "        transformed_audio_tensor = torch.tensor(transformed_audio, dtype=torch.float32).unsqueeze(0).to(audio_waveform.device)\n",
    "\n",
    "        return transformed_audio_tensor\n",
    "\n",
    "# Example usage\n",
    "sample_audio = torch.randn(1, 22050)  # Simulated audio signal with batch size of 1\n",
    "ltn = LoopTransformationNetwork()\n",
    "transformed_audio = ltn(sample_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DrumLoopVAEStyleGAN(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, output_dim=None):\n",
    "        super(DrumLoopVAEStyleGAN, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, output_dim=output_dim)\n",
    "        self.stylegan = StyleGAN(latent_dim=latent_dim, output_dim=output_dim)\n",
    "        self.melgan = MelGAN()\n",
    "        self.ltn = LoopTransformationNetwork()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, apply_lv_ntf=False):\n",
    "        if apply_lv_ntf:\n",
    "            x, factors = apply_lv_ntf(x)  # Apply LV-NTF\n",
    "\n",
    "        # Encoder step\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # StyleGAN step\n",
    "        generated_mel_spec = self.stylegan(z)\n",
    "\n",
    "        # MelGAN step: Ensure the mel_spec is reshaped to 1 channel\n",
    "        if generated_mel_spec.dim() == 2:\n",
    "            generated_mel_spec = generated_mel_spec.unsqueeze(1)\n",
    "\n",
    "        # Audio generation with MelGAN\n",
    "        audio = self.melgan(generated_mel_spec)\n",
    "\n",
    "        # Apply LTN (Loop Transformations) to generated audio\n",
    "        transformed_audio = self.ltn(audio)\n",
    "\n",
    "        return transformed_audio, generated_mel_spec, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE Loss and GAN Loss Functions\n",
    " The VAE loss function includes both reconstruction loss and KL divergence. It is essential for balancing the quality of reconstruction with ensuring a useful latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def vae_loss(reconstructed_x, x, mu, logvar, kl_weight=1.5):\n",
    "    \"\"\"\n",
    "    VAE loss function combining the reconstruction loss and KL divergence.\n",
    "    \n",
    "    Args:\n",
    "    - reconstructed_x: The output of the VAE's decoder (reconstructed mel-spectrogram).\n",
    "    - x: The input mel-spectrogram.\n",
    "    - mu: Mean of the latent variables.\n",
    "    - logvar: Log variance of the latent variables.\n",
    "    - kl_weight: Weight applied to the KL divergence term (annealed over epochs).\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: The combined VAE loss (reconstruction + KL divergence).\n",
    "    - recon_loss: The MSE reconstruction loss.\n",
    "    - kl_div: The KL divergence term.\n",
    "    \"\"\"\n",
    "    # Ensure dimensions match for reconstruction loss\n",
    "    if reconstructed_x.size(-1) != x.size(-1):\n",
    "        reconstructed_x = reconstructed_x[:, :x.size(-1)]  # Adjust dimensions\n",
    "    \n",
    "    # Reconstruction loss (Mean Squared Error between input and reconstructed output)\n",
    "    recon_loss = F.mse_loss(reconstructed_x, x)\n",
    "\n",
    "    # Clamp logvar to avoid extreme values and ensure stability\n",
    "    logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "    mu = torch.clamp(mu, min=-5, max=5)\n",
    "\n",
    "    # KL divergence loss: Regularization term for the VAE\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl_div = torch.clamp(kl_div, min=0, max=100)  # Avoid extremely high values\n",
    "\n",
    "    # Debug: Print KL divergence if NaN or Inf is detected\n",
    "    if torch.isnan(kl_div) or torch.isinf(kl_div):\n",
    "        print(\"NaN or Inf detected in KL divergence\")\n",
    "\n",
    "    # Return total loss with KL weighting\n",
    "    return recon_loss + kl_weight * kl_div, recon_loss, kl_div\n",
    "\n",
    "\n",
    "def gan_loss(discriminator, real_data, fake_data):\n",
    "    \"\"\"\n",
    "    GAN loss function using binary cross-entropy (BCE) for the discriminator.\n",
    "    \n",
    "    Args:\n",
    "    - discriminator: The discriminator network.\n",
    "    - real_data: Real data (ground truth) for the discriminator to classify.\n",
    "    - fake_data: Generated (fake) data from the generator.\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: The combined loss from real and fake data classifications.\n",
    "    \"\"\"\n",
    "    real_loss = F.binary_cross_entropy(discriminator(real_data), torch.ones_like(real_data))\n",
    "    fake_loss = F.binary_cross_entropy(discriminator(fake_data), torch.zeros_like(fake_data))\n",
    "    \n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoader for Audio Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing trimmed audio files\n",
    "TRIMMED_AUDIO_FOLDER_PATH = \"D:/LV-NTF+LoopGAN/data/FSL10K-trimmed\"\n",
    "\n",
    "# Function to get all audio file paths from the directory\n",
    "def get_audio_file_paths(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively gathers all .wav files in the provided folder.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path: The root folder where the .wav files are located.\n",
    "\n",
    "    Returns:\n",
    "    - List of absolute paths to the audio files.\n",
    "    \"\"\"\n",
    "    audio_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):  # Only select .wav files\n",
    "                audio_paths.append(os.path.abspath(os.path.join(root, file)))\n",
    "    return audio_paths\n",
    "\n",
    "# Load the audio file paths\n",
    "audio_paths = get_audio_file_paths(TRIMMED_AUDIO_FOLDER_PATH)\n",
    "\n",
    "# Custom Dataset class to process audio data\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_paths, metadata, feature_extraction_fn, required_input_dim=128):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.metadata = metadata or {}\n",
    "        self.feature_extraction_fn = feature_extraction_fn\n",
    "        self.required_input_dim = required_input_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_paths[idx]\n",
    "        \n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File not found: {audio_file}\")\n",
    "            return None\n",
    "\n",
    "        # Extract features using the provided feature extraction function\n",
    "        try:\n",
    "            feature_dict = self.feature_extraction_fn(audio_file)\n",
    "            if feature_dict is None:\n",
    "                raise ValueError(\"Feature extraction returned None\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or extracting features from file: {audio_file}, Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract numerical features for model input\n",
    "        scalar_features = np.array([\n",
    "            feature_dict.get('bpm', 0),\n",
    "            feature_dict.get('zero_crossing_rate', 0),\n",
    "            feature_dict.get('spectral_centroid', 0),\n",
    "            feature_dict.get('spectral_bandwidth', 0),\n",
    "            feature_dict.get('rms_energy', 0),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # Combine scalar, MFCCs, and chroma features\n",
    "        mfcc_features = feature_dict.get('mfccs', np.zeros(13))\n",
    "        chroma_features = feature_dict.get('chroma', np.zeros(12))\n",
    "        numeric_features = np.concatenate((scalar_features, mfcc_features, chroma_features)).astype(np.float32)\n",
    "        \n",
    "        # Normalize features and pad/truncate to required input size\n",
    "        features = (numeric_features - numeric_features.min()) / (numeric_features.max() - numeric_features.min() + 1e-6)\n",
    "        if features.size < self.required_input_dim:\n",
    "            features = np.pad(features, (0, self.required_input_dim - features.size), 'constant')\n",
    "        else:\n",
    "            features = features[:self.required_input_dim]\n",
    "        \n",
    "        return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.stack(batch)\n",
    "\n",
    "# Define the dataset and DataLoader\n",
    "audio_dataset = AudioDataset(audio_paths=audio_paths, feature_extraction_fn=extract_audio_features_with_metadata, metadata=metadata)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=audio_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn  # Use the custom collate function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization and Audio Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "def visualize_spectrogram(mel_spec, title=\"Mel-Spectrogram\", sr=22050):\n",
    "    \"\"\"Visualizes a mel-spectrogram.\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def playback_audio(audio, sr=22050):\n",
    "    \"\"\"Plays back the generated audio.\"\"\"\n",
    "    display(Audio(audio, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jehj5akl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a51004308b440ab542d558961b76b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Training VAE-StyleGAN Run</strong> at: <a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN/runs/jehj5akl' target=\"_blank\">https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN/runs/jehj5akl</a><br/> View project at: <a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN' target=\"_blank\">https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240912_123415-jehj5akl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jehj5akl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\LV-NTF+LoopGAN\\wandb\\run-20240912_124453-6zq4txvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN/runs/6zq4txvp' target=\"_blank\">Training VAE-StyleGAN Run</a></strong> to <a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN' target=\"_blank\">https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN/runs/6zq4txvp' target=\"_blank\">https://wandb.ai/anecoicastudio-berlin/VAE-loopGAN/runs/6zq4txvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"c57527a5a2fe25105af1f8467d782bff19d66097\"  \n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train.ipynb\"  # Name your notebook in W&B\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Initialize the W&B project with a unique name for this run\n",
    "wandb.init(project=\"VAE-loopGAN\", name=\"Training VAE-StyleGAN Run\")\n",
    "\n",
    "# Configuration for the experiment (can be logged to W&B for tracking)\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"latent_dim\": 64,\n",
    "    \"kl_weight\": 0.0,  # Start KL weight at 0, gradually increase during training\n",
    "    \"epochs\": 100\n",
    "}\n",
    "\n",
    "# Optionally log the configuration to W&B\n",
    "wandb.config.update(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at vae_stylegan_checkpoint_epoch_10.pth. Starting from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339ae6f451374a72bae27dbcf250aaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/297 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anecoica\\miniconda3\\envs\\drumloop_gen\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m features \u001b[38;5;241m=\u001b[39m (features \u001b[38;5;241m-\u001b[39m features\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (features\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m features\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)  \u001b[38;5;66;03m# Normalize input\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Forward pass through the VAE-StyleGAN\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m audio_output, mel_spec, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mvae_stylegan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[0;32m    124\u001b[0m total_loss, recon_loss, kl_div \u001b[38;5;241m=\u001b[39m vae_loss(mel_spec\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), features, mu, logvar, kl_weight)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Perform gradient clipping to avoid exploding gradients and update weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anecoica\\miniconda3\\envs\\drumloop_gen\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anecoica\\miniconda3\\envs\\drumloop_gen\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 38\u001b[0m, in \u001b[0;36mDrumLoopVAEStyleGAN.forward\u001b[1;34m(self, x, apply_lv_ntf)\u001b[0m\n\u001b[0;32m     35\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmelgan(generated_mel_spec)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Apply LTN (Loop Transformations) to generated audio\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m transformed_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mltn\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_audio, generated_mel_spec, mu, logvar\n",
      "File \u001b[1;32mc:\\Users\\anecoica\\miniconda3\\envs\\drumloop_gen\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anecoica\\miniconda3\\envs\\drumloop_gen\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 22\u001b[0m, in \u001b[0;36mLoopTransformationNetwork.forward\u001b[1;34m(self, audio_waveform, sample_rate)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mApplies time-stretching and pitch-shifting to the audio loop.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m- transformed_audio: Audio after time-stretching and pitch-shifting.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert tensor to NumPy array for librosa\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m audio_np \u001b[38;5;241m=\u001b[39m \u001b[43maudio_waveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Debug: Check the shape of the audio input\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal audio shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "latent_dim = 64\n",
    "input_dim = 128  # Input dimension from the extracted features\n",
    "output_dim = 256  # Output dimension representing the mel-spectrogram size\n",
    "\n",
    "vae_stylegan = DrumLoopVAEStyleGAN(latent_dim=latent_dim, input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(vae_stylegan.parameters(), lr=1e-4)  # Using Adam optimizer\n",
    "\n",
    "# Initialize lists for tracking metrics\n",
    "recon_losses = []  # Track reconstruction losses\n",
    "kl_divs = []  # Track KL divergence losses\n",
    "learning_rates = []  # Track learning rates\n",
    "gradient_norms = []  # Track gradient norms for debugging and monitoring\n",
    "\n",
    "# Function to load the training checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load model checkpoint and resume training from the last saved epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    - checkpoint_path: Path to the checkpoint file.\n",
    "    - model: The VAE model instance.\n",
    "    - optimizer: The optimizer instance (Adam).\n",
    "    \n",
    "    Returns:\n",
    "    - start_epoch: The epoch number to resume training from.\n",
    "    \"\"\"\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Load optimizer state\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "        recon_losses.extend(checkpoint['recon_losses'])\n",
    "        kl_divs.extend(checkpoint['kl_divs'])\n",
    "        learning_rates.extend(checkpoint['learning_rates'])\n",
    "        \n",
    "        if 'gradient_norms' in checkpoint:\n",
    "            gradient_norms.extend(checkpoint['gradient_norms'])\n",
    "        else:\n",
    "            gradient_norms = []  # Initialize empty if not found\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        return 0  # Start from epoch 0 if no checkpoint exists\n",
    "\n",
    "# Function to plot training dynamics\n",
    "def plot_training_dynamics():\n",
    "    \"\"\"\n",
    "    Plot graphs for training dynamics: reconstruction loss, KL divergence, learning rate, and gradient norms.\n",
    "    \"\"\"\n",
    "    epochs = list(range(1, len(recon_losses) + 1))\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot reconstruction loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, recon_losses, label=\"Reconstruction Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Reconstruction Loss\")\n",
    "\n",
    "    # Plot KL divergence\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, kl_divs, label=\"KL Divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL Divergence\")\n",
    "    plt.title(\"KL Divergence\")\n",
    "\n",
    "    # Plot learning rate over time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, learning_rates, label=\"Learning Rate\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Over Time\")\n",
    "\n",
    "    # Plot gradient norms\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, gradient_norms, label=\"Gradient Norm\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Gradient Norm\")\n",
    "    plt.title(\"Gradient Norm Over Time\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load checkpoint if available\n",
    "checkpoint_path = \"vae_stylegan_checkpoint_epoch_10.pth\"\n",
    "start_epoch = load_checkpoint(checkpoint_path, vae_stylegan, optimizer)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Total number of epochs to train\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", dynamic_ncols=True)\n",
    "    vae_stylegan.train()\n",
    "    avg_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Initialize tracking for the current epoch's reconstruction and KL losses\n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_kl_div = 0.0\n",
    "\n",
    "    # Gradual KL weight increase after 10 epochs\n",
    "    kl_weight = min(1.0, (epoch - 10) / 20) if epoch >= 10 else 0\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue  # Skip invalid batches\n",
    "\n",
    "        # Move input features to the device\n",
    "        features = batch.to(device)\n",
    "        features = (features - features.min()) / (features.max() - features.min() + 1e-6)  # Normalize input\n",
    "\n",
    "        # Forward pass through the VAE-StyleGAN\n",
    "        audio_output, mel_spec, mu, logvar = vae_stylegan(features.unsqueeze(1))  # Add channel dimension\n",
    "        # Detach for logging/debugging purposes (optional)\n",
    "        mu_np = mu.detach().cpu().numpy()  # Detach before using NumPy if needed for logging\n",
    "        logvar_np = logvar.detach().cpu().numpy()  # Detach for logging/debugging\n",
    "\n",
    "        total_loss, recon_loss, kl_div = vae_loss(mel_spec.squeeze(1), features, mu, logvar, kl_weight)\n",
    "\n",
    "        # Perform gradient clipping to avoid exploding gradients and update weights\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae_stylegan.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics for tracking\n",
    "        avg_loss += total_loss.item()\n",
    "        batch_count += 1\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_kl_div += kl_div.item()\n",
    "\n",
    "        # Display progress\n",
    "        progress_bar.set_postfix({\n",
    "            \"Batch Loss\": f\"{total_loss.item():.3f}\",\n",
    "            \"Recon Loss\": f\"{recon_loss.item():.3f}\",\n",
    "            \"KL Div\": f\"{kl_div.item():.2f}\",\n",
    "            \"KL Weight\": f\"{kl_weight:.2f}\"\n",
    "        })\n",
    "\n",
    "    # Log metrics to W&B after every epoch\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"Batch Loss\": avg_loss / batch_count,\n",
    "        \"Recon Loss\": epoch_recon_loss / batch_count,\n",
    "        \"KL Div\": epoch_kl_div / batch_count,\n",
    "        \"KL Weight\": kl_weight,\n",
    "        \"mu_mean\": mu.mean().item(),\n",
    "        \"mu_std\": mu.std().item(),\n",
    "        \"logvar_mean\": logvar.mean().item(),\n",
    "        \"logvar_std\": logvar.std().item()\n",
    "    })\n",
    "\n",
    "    # Save model checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae_stylegan.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'recon_losses': recon_losses,\n",
    "            'kl_divs': kl_divs,\n",
    "            'learning_rates': learning_rates,\n",
    "            'gradient_norms': gradient_norms\n",
    "        }\n",
    "        checkpoint_path = f\"vae_stylegan_checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        wandb.save(checkpoint_path)\n",
    "\n",
    "# Plot the training dynamics after completing all epochs\n",
    "plot_training_dynamics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the trained model checkpoint\n",
    "checkpoint_path = \"vae_stylegan_checkpoint_epoch_10.pth\"  # Adjust with your actual checkpoint\n",
    "vae_stylegan = DrumLoopVAEStyleGAN(latent_dim=64, input_dim=128, output_dim=256).to(device)\n",
    "vae_stylegan.eval()\n",
    "\n",
    "# Load the model weights from the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "vae_stylegan.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Function to generate samples from the VAE latent space\n",
    "def generate_samples(num_samples=5, latent_dim=64, n_mels=128, noise_factor=0.0):\n",
    "    \"\"\" Generates audio samples from the VAE-StyleGAN latent space \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Generate latent vectors from normal distribution\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        z = z + torch.randn_like(z) * noise_factor  # Add noise to the latent space\n",
    "        generated_mel_spectrograms = vae_stylegan.decode(z)\n",
    "\n",
    "        # Check the shape of the generated mel-spectrograms\n",
    "        print(f\"Generated mel-spectrograms shape: {generated_mel_spectrograms.shape}\")\n",
    "\n",
    "        # Infer time steps dynamically based on the output size\n",
    "        total_elements = generated_mel_spectrograms.numel()\n",
    "        time_steps = total_elements // (num_samples * n_mels)\n",
    "\n",
    "        if total_elements != num_samples * n_mels * time_steps:\n",
    "            raise ValueError(f\"Cannot reshape tensor of size {total_elements} into shape ({num_samples}, {n_mels}, {time_steps}).\")\n",
    "\n",
    "        # Reshape the generated mel-spectrograms to the correct dimensions\n",
    "        generated_mel_spectrograms = generated_mel_spectrograms.view(num_samples, n_mels, time_steps)\n",
    "\n",
    "        return generated_mel_spectrograms.cpu().numpy()\n",
    "\n",
    "# Function to convert mel-spectrogram to waveform using Griffin-Lim\n",
    "def mel_to_audio(mel_spec, sr=22050, n_fft=32):\n",
    "    \"\"\" Convert mel-spectrogram back to audio using Griffin-Lim algorithm \"\"\"\n",
    "    mel_spec = np.exp(mel_spec)  # Convert log-mel to linear-mel if applicable\n",
    "\n",
    "    # Ensure that the mel spectrogram is properly scaled\n",
    "    if mel_spec.ndim != 2:\n",
    "        raise ValueError(f\"Expected a 2D Mel-spectrogram, got shape: {mel_spec.shape}\")\n",
    "\n",
    "    # Convert mel-spectrogram to audio waveform\n",
    "    audio_waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=n_fft)\n",
    "    return audio_waveform\n",
    "\n",
    "# Generate some samples from the latent space\n",
    "num_samples = 5\n",
    "generated_samples = generate_samples(num_samples)\n",
    "\n",
    "# Ensure the folder exists for saving audio\n",
    "output_dir = r\"D:\\LV-NTF+LoopGAN\\inference_audio\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the waveform to the specified folder\n",
    "import soundfile as sf\n",
    "for i, mel_spec in enumerate(generated_samples):\n",
    "    mel_spec = np.exp(mel_spec)  # Convert log-mel to linear-mel if needed\n",
    "    print(f\"Mel-spectrogram shape: {mel_spec.shape}\")\n",
    "\n",
    "    # Convert the mel-spectrogram back to audio waveform\n",
    "    audio_waveform = mel_to_audio(mel_spec)\n",
    "\n",
    "    # Define the file path for each generated audio file\n",
    "    file_path = os.path.join(output_dir, f\"generated_sample_{i}.wav\")\n",
    "\n",
    "    # Save the waveform to the specified folder\n",
    "    sf.write(file_path, audio_waveform, samplerate=22050)\n",
    "    print(f\"Saved generated audio to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drumloop_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
