{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature\n",
    "\n",
    "1. BPM (Beats per Minute): 90\n",
    "\n",
    "- Purpose: Measures the tempo of the audio file. In drum loops, the BPM is critical for synchronizing and controlling the rhythm.\n",
    "- Use case: This can be used as a conditioning factor for your VAE or StyleGAN models to control the tempo of generated audio loops.\n",
    "\n",
    "2. Zero-Crossing Rate: 0.0148 (approx)\n",
    "\n",
    "- Purpose: Measures how often the signal changes sign. This feature can indicate the noisiness or tonal quality of the audio.\n",
    "- Use case: Can be relevant in distinguishing different types of drum sounds (e.g., harsh/noisy snare vs. smooth bass).\n",
    "\n",
    "\n",
    "3. Spectral Centroid: 796.29\n",
    "\n",
    "- Purpose: Represents the \"brightness\" of the sound by indicating where the center of mass of the spectrum is located.\n",
    "- Use case: Helpful for timbre analysis, especially in distinguishing between bright or dark sound patterns in drum loops.\n",
    "\n",
    "\n",
    "4. Spectral Bandwidth: 1406.69\n",
    "\n",
    "- Purpose: Measures the width of the spectrum. This value gives an idea of the range of frequencies present in the sound.\n",
    "- Use case: Can be useful in assessing the complexity of a sound (e.g., a more complex drum pattern might have a broader bandwidth).\n",
    "\n",
    "\n",
    "5. RMS Energy: 0.1047\n",
    "\n",
    "- Purpose: Measures the loudness or power of the audio signal.\n",
    "- Use case: This can directly inform the energy feature for your model, allowing it to condition audio generation based on loudness levels.\n",
    "\n",
    "\n",
    "6. MFCCs (Mel-Frequency Cepstral Coefficients): A 13-element array\n",
    "\n",
    "- Purpose: Used to represent the timbral texture of the audio. MFCCs capture the shape of the spectral envelope and are widely used in audio and speech processing.\n",
    "- Use case: These coefficients can be directly used to model the timbre of the drum sounds, making them a critical input for VAE or GAN models.\n",
    "\n",
    "\n",
    "7. Chroma Features: A 12-element array\n",
    "\n",
    "- Purpose: Represents the distribution of energy across the 12 pitch classes (similar to musical notes).\n",
    "- Use case: Can be used to analyze harmonic content or pitch distribution, although for drums, this will typically show lower values compared to harmonic instruments.\n",
    "\n",
    "\n",
    "8. Mood: 'creative'\n",
    "\n",
    "- Purpose: Mapped from metadata (tags). This is a subjective feature derived from the audio's timbre and overall texture, which can be conditioned to generate different emotional flavors in drum loops.\n",
    "- Use case: This feature can serve as a high-level condition for generating different types of drum loops (e.g., happy, dark, energetic, calm).\n",
    "\n",
    "\n",
    "9. Energy: 0.9\n",
    "\n",
    "- Purpose: Inferred from the tags and calculated from the spectral properties. It reflects how powerful or intense the drum loop is.\n",
    "- Use case: Energy levels can be conditioned to control the intensity of generated drum loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def map_tags_to_features(tags):\n",
    "    \"\"\"\n",
    "    Maps tags to high-level features such as mood, energy, etc.\n",
    "    \"\"\"\n",
    "    mood_mapping = {\n",
    "        'Ambient': 'calm',\n",
    "        'Cinematic': 'epic',\n",
    "        'Looping': 'neutral',\n",
    "        'Drone': 'dark',\n",
    "        'Atmosphere': 'calm',\n",
    "        'Pad': 'soft',\n",
    "        'Drums': 'energetic',\n",
    "        'Sound-Design': 'creative'\n",
    "    }\n",
    "    \n",
    "    energy_mapping = {\n",
    "        'Drums': 0.9,\n",
    "        'Cinematic': 0.8,\n",
    "        'Looping': 0.6,\n",
    "        'Drone': 0.4,\n",
    "        'Pad': 0.5,\n",
    "        'Ambient': 0.3\n",
    "    }\n",
    "    \n",
    "    # Default values\n",
    "    mood = 'neutral'\n",
    "    energy = 0.5\n",
    "    \n",
    "    for tag in tags:\n",
    "        if tag in mood_mapping:\n",
    "            mood = mood_mapping[tag]\n",
    "        if tag in energy_mapping:\n",
    "            energy = max(energy, energy_mapping[tag])\n",
    "    \n",
    "    return mood, energy\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_audio_features_with_metadata(audio_path, sr=22050, metadata=None):\n",
    "    \"\"\"\n",
    "    Extracts audio features using Librosa and combines them with metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_path (str): Path to the audio file.\n",
    "    sr (int): Sampling rate.\n",
    "    metadata (dict): High-level feature metadata.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of extracted audio and metadata features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Print the path being loaded\n",
    "        # print(f\"Trying to load audio file: {audio_path}\")\n",
    "        if os.path.exists(audio_path):\n",
    "            y, sr = librosa.load(audio_path, sr=sr)\n",
    "            # print(\"Audio file loaded successfully\")\n",
    "        else:\n",
    "            print(f\"File not found: {audio_path}\")\n",
    "            return None\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Extract low-level audio features with Librosa\n",
    "        bpm, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        if isinstance(bpm, (np.ndarray, np.generic)):  # Only call .item() on NumPy scalars\n",
    "            bpm = bpm.item()  # Convert to scalar if it's a single-item array\n",
    "        # print(f\"Extracted BPM: {bpm} (Type: {type(bpm)})\")\n",
    "        features['bpm'] = bpm\n",
    "\n",
    "        # Other features (make sure they are floats and not arrays)\n",
    "        features['zero_crossing_rate'] = float(np.mean(librosa.feature.zero_crossing_rate(y)))\n",
    "        features['spectral_centroid'] = float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))\n",
    "        features['spectral_bandwidth'] = float(np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)))\n",
    "        features['rms_energy'] = float(np.mean(librosa.feature.rms(y=y)))\n",
    "        # print(f\"Extracted Zero Crossing Rate: {features['zero_crossing_rate']} (Type: {type(features['zero_crossing_rate'])})\")\n",
    "        \n",
    "        # MFCCs and Chroma features are arrays\n",
    "        features['mfccs'] = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)  # 13 MFCC coefficients\n",
    "        features['chroma'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)  # 12 chroma values\n",
    "        # print(f\"Extracted MFCCs: {features['mfccs']} (Type: {type(features['mfccs'])})\")\n",
    "        # print(f\"Extracted Chroma: {features['chroma']} (Type: {type(features['chroma'])})\")\n",
    "\n",
    "        # Map metadata to relevant features\n",
    "        if metadata:\n",
    "            features.update({\n",
    "                'kick_intensity': metadata.get('Kick', 0),\n",
    "                'snare_intensity': metadata.get('Snare', 0),\n",
    "                'hihat_presence': metadata.get('Hi-hat', 0),\n",
    "                'percussion': metadata.get('Percussion', 0),\n",
    "                'mood': metadata.get('Mood', 'neutral'),\n",
    "                'timbre': metadata.get('Timbre', 'balanced'),\n",
    "                'energy': metadata.get('Energy', 0),\n",
    "                'danceability': metadata.get('Danceability', 0),\n",
    "                'global_loudness': metadata.get('Global Loudness', 0),\n",
    "                'dynamic_complexity': metadata.get('Dynamic Complexity', 0),\n",
    "                'valence': metadata.get('Valence', 'neutral'),\n",
    "                'arousal': metadata.get('Arousal', 'calm'),\n",
    "                'pitch_confidence': metadata.get('Pitch Confidence', 0),\n",
    "                'harmonicity': metadata.get('Harmonicity', 0),\n",
    "                'texture_density': metadata.get('Texture Density', 0),\n",
    "                'articulation': metadata.get('Articulation', 'smooth')\n",
    "            })\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or extracting features from file: {audio_path}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def print_feature_info(features):\n",
    "    \"\"\"\n",
    "    Print detailed information about extracted features.\n",
    "    \n",
    "    Parameters:\n",
    "    features (dict): Extracted audio features.\n",
    "    \"\"\"\n",
    "    for key, value in features.items():\n",
    "        print(f\"Feature: {key}\")\n",
    "        print(f\"Type: {type(value)}\")\n",
    "        \n",
    "        # Check if it's a scalar value\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"Value: {value}\")\n",
    "        \n",
    "        # If it's an array or list, print the shape and first few values\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"Shape: {value.shape}\")\n",
    "            print(f\"First few values: {value[:5]}\")  # Display the first 5 elements if it's an array\n",
    "            \n",
    "        # For other types, print directly\n",
    "        else:\n",
    "            print(f\"Value: {value}\")\n",
    "        \n",
    "        print(\"\\n\")  # Separate outputs for clarity\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "audio_path = \"D:/LV-NTF+LoopGAN/data/FSL10K-trimmed/505_301.wav\"\n",
    "metadata = {\n",
    "    \"Kick\": 0.8, \n",
    "    \"Snare\": 0.7, \n",
    "    \"Hi-hat\": 0.9, \n",
    "    \"Percussion\": 0.6, \n",
    "    \"Mood\": \"happy\", \n",
    "    \"Timbre\": \"bright\", \n",
    "    \"Energy\": 0.85, \n",
    "    \"Danceability\": 0.9\n",
    "}\n",
    "\n",
    "# Extract features\n",
    "audio_features = extract_audio_features_with_metadata(audio_path, metadata=metadata)\n",
    "\n",
    "# Print detailed info about the features\n",
    "# print_feature_info(audio_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Encoder and Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN (Mel-spectrogram Generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MelGAN (Mel-Spectrogram to Audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelGAN, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(1, 128, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose1d(64, 1, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input has a single channel before the transposed convolution\n",
    "        if x.shape[1] != 1:  # If it doesn't have 1 channel\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension, from [batch_size, time_steps] -> [batch_size, 1, time_steps]\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return torch.tanh(self.conv3(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE-GAN Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumLoopVAEStyleGAN(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super(DrumLoopVAEStyleGAN, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, output_dim=output_dim)\n",
    "        self.stylegan = StyleGAN(latent_dim=latent_dim, output_dim=output_dim)\n",
    "        self.melgan = MelGAN()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        std = torch.clamp(std, min=1e-6)  # Avoid too small std\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder step\n",
    "        mu, logvar = self.encoder(x)\n",
    "        # print(f\"mu: min {mu.min().item()}, max {mu.max().item()}, mean {mu.mean().item()}, std {mu.std().item()}\")\n",
    "        # print(f\"logvar: min {logvar.min().item()}, max {logvar.max().item()}, mean {logvar.mean().item()}, std {logvar.std().item()}\")\n",
    "\n",
    "        # Reparameterize to sample from latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # Generate mel spectrogram with StyleGAN\n",
    "        generated_mel_spec = self.stylegan(z)\n",
    "        \n",
    "        # Ensure that the mel_spec is reshaped to have 1 channel before feeding it to MelGAN\n",
    "        if generated_mel_spec.dim() == 2:  # If it is [batch_size, time_steps]\n",
    "            generated_mel_spec = generated_mel_spec.unsqueeze(1)  # Add a channel dimension\n",
    "\n",
    "        # Generate audio with MelGAN\n",
    "        audio = self.melgan(generated_mel_spec)\n",
    "        return audio, generated_mel_spec, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions (VAE and GAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def vae_loss(reconstructed_x, x, mu, logvar, kl_weight=1.5):\n",
    "    # Ensure the reconstructed_x (mel_spec) and x (features) have the same dimensions\n",
    "    if reconstructed_x.size(-1) != x.size(-1):\n",
    "        reconstructed_x = reconstructed_x[:, :x.size(-1)]  # Match length to input features\n",
    "\n",
    "    # Reconstruction loss (MSE between reconstructed mel_spec and input features)\n",
    "    recon_loss = nn.MSELoss()(reconstructed_x, x)\n",
    "\n",
    "    # Clamp logvar to avoid extremely large or small values but allow variability\n",
    "    logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "    mu = torch.clamp(mu, min=-5, max=5)\n",
    "\n",
    "    # KL divergence loss (regularization term for the VAE)\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl_div = torch.clamp(kl_div, min=0, max=100)  # Clip to avoid exploding values\n",
    "\n",
    "    # Print for debugging if needed\n",
    "    # print(f\"recon_loss: {recon_loss.item()}, kl_div: {kl_div.item()}\")\n",
    "\n",
    "    if torch.isnan(kl_div) or torch.isinf(kl_div):\n",
    "        print(\"NaN or Inf detected in KL divergence\")\n",
    "\n",
    "    # Return total loss with KL weighting\n",
    "    return recon_loss + kl_weight * kl_div, recon_loss, kl_div\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gan_loss(discriminator, real_data, fake_data):\n",
    "    real_loss = nn.BCELoss()(discriminator(real_data), torch.ones_like(real_data))\n",
    "    fake_loss = nn.BCELoss()(discriminator(fake_data), torch.zeros_like(fake_data))\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Data Loader (train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing trimmed audio files\n",
    "TRIMMED_AUDIO_FOLDER_PATH = \"D:/LV-NTF+LoopGAN/data/FSL10K-trimmed\"\n",
    "\n",
    "# Function to get all audio file paths from the directory\n",
    "def get_audio_file_paths(folder_path):\n",
    "    audio_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):  # Ensure only .wav files are picked\n",
    "                audio_paths.append(os.path.abspath(os.path.join(root, file)))  # Convert to absolute path\n",
    "    return audio_paths\n",
    "\n",
    "# Load the audio file paths\n",
    "audio_paths = get_audio_file_paths(TRIMMED_AUDIO_FOLDER_PATH)\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_paths, metadata, feature_extraction_fn, required_input_dim=128):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.metadata = metadata or {}\n",
    "        self.feature_extraction_fn = feature_extraction_fn\n",
    "        self.required_input_dim = required_input_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_paths[idx]\n",
    "        \n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File not found: {audio_file}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            feature_dict = self.feature_extraction_fn(audio_file)\n",
    "            if feature_dict is None:\n",
    "                raise ValueError(\"Feature extraction returned None\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or extracting features from file: {audio_file}, Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract relevant numeric features from the feature_dict\n",
    "        scalar_features = np.array([\n",
    "            feature_dict.get('bpm', 0),\n",
    "            feature_dict.get('zero_crossing_rate', 0),\n",
    "            feature_dict.get('spectral_centroid', 0),\n",
    "            feature_dict.get('spectral_bandwidth', 0),\n",
    "            feature_dict.get('rms_energy', 0),\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Combine MFCCs and chroma features with scalar features\n",
    "        mfcc_features = feature_dict.get('mfccs', np.zeros(13))\n",
    "        chroma_features = feature_dict.get('chroma', np.zeros(12))\n",
    "\n",
    "        numeric_features = np.concatenate((scalar_features, mfcc_features, chroma_features)).astype(np.float32)\n",
    "        \n",
    "        # Normalize the combined features\n",
    "        features = (numeric_features - numeric_features.min()) / (numeric_features.max() - numeric_features.min() + 1e-6)\n",
    "\n",
    "        # Ensure the features are padded or truncated to the required input dimension\n",
    "        if features.size < self.required_input_dim:\n",
    "            features = np.pad(features, (0, self.required_input_dim - features.size), 'constant')\n",
    "        else:\n",
    "            features = features[:self.required_input_dim]\n",
    "        \n",
    "        return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None items from the batch\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return None  # Return None if all elements in the batch were None\n",
    "\n",
    "    # Stack valid items\n",
    "    return torch.stack(batch)\n",
    "\n",
    "# Define dataset and dataloader\n",
    "audio_dataset = AudioDataset(audio_paths=audio_paths, feature_extraction_fn=extract_audio_features_with_metadata, metadata=metadata)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=audio_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn  # Use the custom collate function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "def visualize_spectrogram(mel_spec, title=\"Mel-Spectrogram\", sr=22050):\n",
    "    \"\"\"Visualizes a mel-spectrogram.\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def playback_audio(audio, sr=22050):\n",
    "    \"\"\"Plays back the generated audio in Jupyter.\"\"\"\n",
    "    display(Audio(audio, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm  # Using autonotebook for Jupyter/VScode compatibility\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model and optimizer\n",
    "latent_dim = 64\n",
    "input_dim = 128  # Adjust based on extracted features\n",
    "output_dim = 256  # Mel-spectrogram size\n",
    "\n",
    "vae_stylegan = DrumLoopVAEStyleGAN(latent_dim=latent_dim, input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(vae_stylegan.parameters(), lr=1e-4)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "recon_losses = []\n",
    "kl_divs = []\n",
    "learning_rates = []\n",
    "gradient_norms = []\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "        recon_losses.extend(checkpoint['recon_losses'])  # Continue tracking losses\n",
    "        kl_divs.extend(checkpoint['kl_divs'])\n",
    "        learning_rates.extend(checkpoint['learning_rates'])\n",
    "        gradient_norms.extend(checkpoint['gradient_norms'])\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        return 0  # Start from the beginning if no checkpoint found\n",
    "\n",
    "# Function to plot training dynamics\n",
    "def plot_training_dynamics():\n",
    "    epochs = list(range(1, len(recon_losses) + 1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Reconstruction loss plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, recon_losses, label=\"Reconstruction Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Reconstruction Loss\")\n",
    "    \n",
    "    # KL divergence plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, kl_divs, label=\"KL Divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL Divergence\")\n",
    "    plt.title(\"KL Divergence\")\n",
    "    \n",
    "    # Learning rate plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, learning_rates, label=\"Learning Rate\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Over Time\")\n",
    "    \n",
    "    # Gradient norm plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, gradient_norms, label=\"Gradient Norm\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Gradient Norm\")\n",
    "    plt.title(\"Gradient Norm Over Time\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize the spectrogram\n",
    "def visualize_spectrogram(mel_spec, title=\"Mel-Spectrogram\", sr=22050):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to play audio\n",
    "def playback_audio(audio, sr=22050):\n",
    "    display(Audio(audio, rate=sr))\n",
    "\n",
    "# Path to your checkpoint\n",
    "checkpoint_path = \"vae_stylegan_checkpoint_epoch_10.pth\"  # Adjust as necessary\n",
    "\n",
    "# Load checkpoint if available and resume training\n",
    "start_epoch = load_checkpoint(checkpoint_path, vae_stylegan, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", dynamic_ncols=True)\n",
    "    \n",
    "    vae_stylegan.train()\n",
    "    avg_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_kl_div = 0.0\n",
    "    epoch_mu_stats = {'min': float('inf'), 'max': float('-inf'), 'mean': 0.0, 'std': 0.0}\n",
    "    epoch_logvar_stats = {'min': float('inf'), 'max': float('-inf'), 'mean': 0.0, 'std': 0.0}\n",
    "\n",
    "    # Delay KL weight to allow focus on reconstruction first\n",
    "    if epoch < 10:\n",
    "        kl_weight = 0\n",
    "    else:\n",
    "        kl_weight = min(1.0, (epoch - 10) / 20)\n",
    "\n",
    "    \"\"\"\n",
    "    If you prefer a more complex strategy, try cyclical KL annealing \n",
    "    where the weight oscillates between 0 and 1 to emphasize reconstruction \n",
    "    in some phases and regularization in others.\n",
    "    Cyclical KL weight annealing \n",
    "    kl_weight = 0.5 * (1 + np.cos(np.pi * (epoch % 20) / 20))           \n",
    "    \"\"\"\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if batch is None:\n",
    "            continue  # Skip invalid batch\n",
    "\n",
    "        features = batch.to(device)\n",
    "\n",
    "        # Normalize input features to avoid extreme values\n",
    "        features = (features - features.min()) / (features.max() - features.min() + 1e-6)\n",
    "\n",
    "        # Forward pass through the VAE-StyleGAN model\n",
    "        audio_output, mel_spec, mu, logvar = vae_stylegan(features.unsqueeze(1))\n",
    "\n",
    "        # Compute the VAE loss\n",
    "        total_loss, recon_loss, kl_div = vae_loss(mel_spec.squeeze(1), features, mu, logvar, kl_weight)\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(vae_stylegan.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += total_loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # Accumulate reconstruction loss, KL divergence for this epoch\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_kl_div += kl_div.item()\n",
    "\n",
    "        # Update epoch-wise stats for mu and logvar\n",
    "        epoch_mu_stats['min'] = min(epoch_mu_stats['min'], mu.min().item())\n",
    "        epoch_mu_stats['max'] = max(epoch_mu_stats['max'], mu.max().item())\n",
    "        epoch_mu_stats['mean'] += mu.mean().item()\n",
    "        epoch_mu_stats['std'] += mu.std().item()\n",
    "\n",
    "        epoch_logvar_stats['min'] = min(epoch_logvar_stats['min'], logvar.min().item())\n",
    "        epoch_logvar_stats['max'] = max(epoch_logvar_stats['max'], logvar.max().item())\n",
    "        epoch_logvar_stats['mean'] += logvar.mean().item()\n",
    "        epoch_logvar_stats['std'] += logvar.std().item()\n",
    "\n",
    "        # Update the progress bar with basic info for each batch\n",
    "        progress_bar.set_postfix({\n",
    "            \"Batch Loss\": f\"{total_loss.item():.3f}\",\n",
    "            \"Recon Loss\": f\"{recon_loss.item():.3f}\",\n",
    "            \"KL Div\": f\"{kl_div.item():.2f}\",\n",
    "            \"Avg Loss\": f\"{avg_loss / batch_count:.3f}\",\n",
    "            \"KL Weight\": f\"{kl_weight:.2f}\"\n",
    "        })\n",
    "\n",
    "    # Compute mean statistics for mu and logvar for the entire epoch\n",
    "    epoch_mu_stats['mean'] /= batch_count\n",
    "    epoch_mu_stats['std'] /= batch_count\n",
    "    epoch_logvar_stats['mean'] /= batch_count\n",
    "    epoch_logvar_stats['std'] /= batch_count\n",
    "\n",
    "    # Record metrics for the current epoch\n",
    "    recon_losses.append(epoch_recon_loss / batch_count)\n",
    "    kl_divs.append(epoch_kl_div / batch_count)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # End of epoch: log detailed metrics for the epoch\n",
    "    tqdm.write(f\"mu: min {epoch_mu_stats['min']:.5f}, max {epoch_mu_stats['max']:.5f}, mean {epoch_mu_stats['mean']:.5f}, std {epoch_mu_stats['std']:.5f}\")\n",
    "    tqdm.write(f\"logvar: min {epoch_logvar_stats['min']:.5f}, max {epoch_logvar_stats['max']:.5f}, mean {epoch_logvar_stats['mean']:.5f}, std {epoch_logvar_stats['std']:.5f}\")\n",
    "    tqdm.write(f\"recon_loss: {epoch_recon_loss / batch_count:.5f}, kl_div: {epoch_kl_div / batch_count:.5f}\")\n",
    "\n",
    "    \n",
    "    # Save model checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae_stylegan.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'recon_losses': recon_losses,\n",
    "            'kl_divs': kl_divs,\n",
    "            'learning_rates': learning_rates\n",
    "        }, f\"vae_stylegan_checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "# Plot the training dynamics after training\n",
    "plot_training_dynamics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for evaluation\n",
    "- reconstruction loss\n",
    "- Frechet Audio Distance (FAD)\n",
    "- Inception Score\n",
    "- Visual Inspection of Mel-Spectrogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drumloop_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
